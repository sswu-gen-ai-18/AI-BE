# server/agents/emotion_agent.py
import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from huggingface_hub import snapshot_download  # HFì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

# ğŸ”¹ Hugging Faceì— ì˜¬ë¦° ë„¤ ëª¨ë¸ ë¦¬í¬ ì´ë¦„
MODEL_REPO = "hozziii/kobert-emotion-final"

# ğŸ”¹ AI-BE/server/agents ê¸°ì¤€ìœ¼ë¡œ í•œ ë‹¨ê³„ ì˜¬ë¼ê°€ì„œ(models í´ë” ì°¾ê¸°)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_DIR = os.path.join(BASE_DIR, "models", "kobert_emotion_final")

# ğŸ”¹ ë¡œì»¬ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì¤€ë¹„ & ì—†ìœ¼ë©´ HFì—ì„œ ë°›ì•„ì˜¤ê¸°
os.makedirs(MODEL_DIR, exist_ok=True)
local_model_file = os.path.join(MODEL_DIR, "model.safetensors")

if not os.path.exists(local_model_file):
    print("[EmotionAgent] ë¡œì»¬ì— KoBERT ê°ì • ëª¨ë¸ì´ ì—†ì–´ HFì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
    snapshot_download(
        repo_id=MODEL_REPO,
        local_dir=MODEL_DIR,
        ignore_patterns=["*.msgpack"],  # ì„ íƒ ì˜µì…˜
    )
    print("[EmotionAgent] ë‹¤ìš´ë¡œë“œ ì™„ë£Œ:", MODEL_DIR)

# ğŸ”¹ í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained(
    "monologg/kobert",
    trust_remote_code=True,
)

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_DIR,              # HFì—ì„œ ë°›ì€ í´ë”ì—ì„œ ë°”ë¡œ ë¡œë”©
    local_files_only=True,
    trust_remote_code=True,
)
model.eval()

# ğŸ”¹ id2label ì„¤ì • (ëª¨ë¸ configì— ìˆìœ¼ë©´ ê·¸ê²ƒ ìš°ì„ )
if hasattr(model.config, "id2label") and model.config.id2label:
    id2label = {int(k): v for k, v in model.config.id2label.items()}
else:
    # ê¸°ë³¸ ë§¤í•‘ (ëª¨ë¸ì´ 3-classë¼ê³  ê°€ì •)
    id2label = {0: "anger", 1: "sad", 2: "fear"}


# âœ… ì¸ì‚¿ë§/í˜•ì‹ ë©˜íŠ¸ íŒ¨í„´ (ë¬´ì¡°ê±´ neutralë¡œ ì²˜ë¦¬í•  í›„ë³´ë“¤)
GREETING_PATTERNS = [
    "ì•ˆë…•í•˜ì„¸ìš”",
    "ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ",
    "ì „í™”ë“œë ¸ìŠµë‹ˆë‹¤",
    "ì „í™” ë“œë ¸ìŠµë‹ˆë‹¤",
    "ë„ì™€ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤",
    "ë„ì™€ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤",
    "ê°ì‚¬í•©ë‹ˆë‹¤",
    "ìˆ˜ê³ í•˜ì„¸ìš”",
    "ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤",
]

# âœ… ê°ì • í™•ì‹ ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ neutralë¡œ ëŒë¦¬ëŠ” threshold
NEUTRAL_THRESHOLD = 0.55


class EmotionAgent:
    """
    KoBERT ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ì—ì´ì „íŠ¸
    """

    # ëŒ€í‘œ ê°ì • 1ê°œë§Œ ë°˜í™˜
    def predict(self, text: str) -> dict:
        # -----------------------------
        # 0) ì¸ì‚¿ë§/í˜•ì‹ ë©˜íŠ¸ íœ´ë¦¬ìŠ¤í‹±
        # -----------------------------
        cleaned = text.strip()
        no_space = cleaned.replace(" ", "")

        for pattern in GREETING_PATTERNS:
            if pattern.replace(" ", "") in no_space:
                # ì¸ì‚¿ë§ë¥˜ëŠ” ê°•í•œ ê°ì •ì´ ì—†ë‹¤ê³  ë³´ê³  neutral ê³ ì •
                return {
                    "emotion_label": "neutral",
                    "emotion_score": 0.7,  # ì ë‹¹í•œ ì¤‘ê°„ê°’
                }

        # -----------------------------
        # 1) KoBERT ëª¨ë¸ ì¶”ë¡ 
        # -----------------------------
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=128,
        )
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)
            score, idx = torch.max(probs, dim=1)

        score_val = float(score)
        label = id2label[int(idx.item())]

        # -----------------------------
        # 2) í™•ì‹ ë„ê°€ ë‚®ìœ¼ë©´ neutralë¡œ ê°•ë“±
        # -----------------------------
        if score_val < NEUTRAL_THRESHOLD:
            return {
                "emotion_label": "neutral",
                "emotion_score": score_val,
            }

        # -----------------------------
        # 3) ì¼ë°˜ì ì¸ ê°ì • ê²°ê³¼ ë°˜í™˜
        # -----------------------------
        return {
            "emotion_label": label,
            "emotion_score": score_val,
        }

    # anger, sad, fear ì „ì²´ í™•ë¥  ë°˜í™˜ (ê·¸ë˜í”„ìš©)
    def predict_proba(self, text: str) -> dict:
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=128,
        )
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)[0].tolist()

        # ì—¬ê¸°ì„œëŠ” ì—¬ì „íˆ ëª¨ë¸ì´ ê°€ì§„ 3ê°œ í´ë˜ìŠ¤ ë¶„í¬ ê·¸ëŒ€ë¡œ ë°˜í™˜
        # (í”„ë¡ íŠ¸ ê·¸ë˜í”„ìš©)
        return {
            "anger": float(probs[0]),
            "sad": float(probs[1]),
            "fear": float(probs[2]),
        }
