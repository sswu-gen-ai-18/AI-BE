# server/agents/emotion_agent.py
import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from huggingface_hub import snapshot_download

MODEL_REPO = "hozziii/kobert-emotion-final"

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_DIR = os.path.join(BASE_DIR, "models", "kobert_emotion_final")

os.makedirs(MODEL_DIR, exist_ok=True)
local_model_file = os.path.join(MODEL_DIR, "model.safetensors")

if not os.path.exists(local_model_file):
    print("[EmotionAgent] ë¡œì»¬ì— KoBERT ê°ì • ëª¨ë¸ì´ ì—†ì–´ HFì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
    snapshot_download(
        repo_id=MODEL_REPO,
        local_dir=MODEL_DIR,
        ignore_patterns=["*.msgpack"],
    )
    print("[EmotionAgent] ë‹¤ìš´ë¡œë“œ ì™„ë£Œ:", MODEL_DIR)

tokenizer = AutoTokenizer.from_pretrained(
    "monologg/kobert",
    trust_remote_code=True,
)

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_DIR,
    local_files_only=True,
    trust_remote_code=True,
)
model.eval()

if hasattr(model.config, "id2label") and model.config.id2label:
    id2label = {int(k): v for k, v in model.config.id2label.items()}
else:
    id2label = {0: "anger", 1: "sad", 2: "fear"}


class EmotionAgent:
    """
    KoBERT ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ì—ì´ì „íŠ¸
    """

    # ğŸ”¹ ëŒ€í‘œ ê°ì • 1ê°œë§Œ ë°˜í™˜ (+ ì¸ì‚¬/ì¤‘ë¦½ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ neutral ì²˜ë¦¬)
    def predict(self, text: str) -> dict:
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=128,
        )
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)
            score_tensor, idx_tensor = torch.max(probs, dim=1)

        label = id2label[int(idx_tensor.item())]
        score = float(score_tensor.item())

        # -------------------------------
        # ğŸ”¸ ì¸ì‚¬/ì¤‘ë¦½ ë¬¸ì¥ íœ´ë¦¬ìŠ¤í‹± ì²˜ë¦¬
        # -------------------------------
        clean = (text or "").strip().replace(" ", "")

        greeting_phrases = [
            "ì•ˆë…•í•˜ì„¸ìš”",
            "ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ",
            "ì—¬ë³´ì„¸ìš”",
            "ê°ì‚¬í•©ë‹ˆë‹¤",
            "ìˆ˜ê³ í•˜ì„¸ìš”",
            "ë„¤", "ì˜ˆ", "ì•Œê² ì–´ìš”",
        ]

        is_greeting = any(p in clean for p in greeting_phrases)
        is_very_short = len(clean) <= 3        # ë„ˆë¬´ ì§§ì€ ë‹¨ë‹µ
        low_confidence = score < 0.6          # ëª¨ë¸ í™•ì‹ ë„ ë‚®ì„ ë•Œë§Œ ì¤‘ë¦½ìœ¼ë¡œ ë®ì–´ì“°ê¸°

        if (is_greeting or is_very_short) and low_confidence:
            # ğŸ‘‰ ì´ëŸ° ê²½ìš°ëŠ” ê·¸ëƒ¥ neutral ë¡œ ê°•ì œ ìºìŠ¤íŒ…
            return {
                "emotion_label": "neutral",
                "emotion_score": score,  # í˜¹ì€ 0.0 ìœ¼ë¡œ ê³ ì •í•´ë„ ë¨
            }

        # ê¸°ë³¸: KoBERT ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        return {
            "emotion_label": label,
            "emotion_score": score,
        }

    # anger, sad, fear ì „ì²´ í™•ë¥  ë°˜í™˜ (ê·¸ë˜í”„ìš©)
    def predict_proba(self, text: str) -> dict:
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=128,
        )
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)[0].tolist()

        return {
            "anger": float(probs[0]),
            "sad": float(probs[1]),
            "fear": float(probs[2]),
        }
